{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries\n",
    "import glob\n",
    "from multiprocessing import cpu_count\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "## 3rd party\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFilter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "_path = \"..\"\n",
    "if _path not in sys.path:\n",
    "    sys.path.append(_path)\n",
    "from lib.dataset import TextArtDataLoader, AlignCollate, ImageBatchSampler\n",
    "from lib.config import Config\n",
    "from lib.arch import Generator, Discriminator\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "# N_WORKERS = cpu_count() - 1\n",
    "N_WORKERS = 4\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "N_EPOCHS = 10\n",
    "LR_G = 1e-4\n",
    "LR_D = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "CONFIG = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextArtDataLoader('united',\n",
    "                                  CONFIG.WORD2VEC_MODEL_FILE,\n",
    "                                  mode='train',\n",
    "                                  load_word_vectors=CONFIG.LOAD_WORD_VECTORS)\n",
    "val_dataset = TextArtDataLoader('united',\n",
    "                                CONFIG.WORD2VEC_MODEL_FILE,\n",
    "                                mode='val',\n",
    "                                load_word_vectors=CONFIG.LOAD_WORD_VECTORS)\n",
    "test_dataset = TextArtDataLoader('united',\n",
    "                                 CONFIG.WORD2VEC_MODEL_FILE,\n",
    "                                 mode='test',\n",
    "                                 load_word_vectors=CONFIG.LOAD_WORD_VECTORS)\n",
    "\n",
    "train_align_collate = AlignCollate('train',\n",
    "                                   CONFIG.WORD2VEC_MODEL_FILE,\n",
    "                                   CONFIG.MEAN,\n",
    "                                   CONFIG.STD,\n",
    "                                   CONFIG.IMAGE_SIZE_HEIGHT,\n",
    "                                   CONFIG.IMAGE_SIZE_WIDTH,\n",
    "                                   horizontal_flipping=CONFIG.HORIZONTAL_FLIPPING,\n",
    "                                   random_rotation=CONFIG.RANDOM_ROTATION,\n",
    "                                   color_jittering=CONFIG.COLOR_JITTERING,\n",
    "                                   random_grayscale=CONFIG.RANDOM_GRAYSCALE,\n",
    "                                   random_channel_swapping=CONFIG.RANDOM_CHANNEL_SWAPPING,\n",
    "                                   random_gamma=CONFIG.RANDOM_GAMMA,\n",
    "                                   random_resolution=CONFIG.RANDOM_RESOLUTION,\n",
    "                                   word_vectors_similar_pad=CONFIG.WORD_VECTORS_SIMILAR_PAD,\n",
    "                                   word_vectors_similar_pad_topN=CONFIG.WORD_VECTORS_SIMILAR_PAD_TOPN)\n",
    "val_align_collate = AlignCollate('val',\n",
    "                                 CONFIG.WORD2VEC_MODEL_FILE,\n",
    "                                 CONFIG.MEAN,\n",
    "                                 CONFIG.STD,\n",
    "                                 CONFIG.IMAGE_SIZE_HEIGHT,\n",
    "                                 CONFIG.IMAGE_SIZE_WIDTH,\n",
    "                                 horizontal_flipping=CONFIG.HORIZONTAL_FLIPPING,\n",
    "                                 random_rotation=CONFIG.RANDOM_ROTATION,\n",
    "                                 color_jittering=CONFIG.COLOR_JITTERING,\n",
    "                                 random_grayscale=CONFIG.RANDOM_GRAYSCALE,\n",
    "                                 random_channel_swapping=CONFIG.RANDOM_CHANNEL_SWAPPING,\n",
    "                                 random_gamma=CONFIG.RANDOM_GAMMA,\n",
    "                                 random_resolution=CONFIG.RANDOM_RESOLUTION,\n",
    "                                 word_vectors_similar_pad=CONFIG.WORD_VECTORS_SIMILAR_PAD,\n",
    "                                 word_vectors_similar_pad_topN=CONFIG.WORD_VECTORS_SIMILAR_PAD_TOPN)\n",
    "\n",
    "train_batch_sampler = ImageBatchSampler('united', BATCH_SIZE, shuffle_groups=True, mode='train')\n",
    "val_batch_sampler = ImageBatchSampler('united', BATCH_SIZE, shuffle_groups=True, mode='val')\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False,\n",
    "                          num_workers=N_WORKERS,\n",
    "                          pin_memory=True,\n",
    "                          collate_fn=train_align_collate,\n",
    "                          sampler=train_batch_sampler,\n",
    "                         )\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False,\n",
    "                          num_workers=N_WORKERS,\n",
    "                          pin_memory=True,\n",
    "                          collate_fn=val_align_collate,\n",
    "                          sampler=val_batch_sampler\n",
    "                         )\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False,\n",
    "                          num_workers=N_WORKERS,\n",
    "                          pin_memory=True,\n",
    "                          collate_fn=None,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i, (image, word_vectors_tensor) in enumerate(train_loader):\n",
    "#     print(\"IMAGE:\", image.shape)\n",
    "#     print(\"WV:\", word_vectors_tensor.shape)\n",
    "    \n",
    "#     for word_vectors in word_vectors_tensor:\n",
    "#         for word_vector in word_vectors:\n",
    "#             wv = np.array(word_vector)\n",
    "#             word, prob = train_loader.dataset.word2vec_model.wv.similar_by_vector(wv)[0]\n",
    "#             if prob > 0.95:\n",
    "#                 print(word, end=',')\n",
    "#         print()\n",
    "    \n",
    "#     images = np.array(image)\n",
    "#     for img in images:\n",
    "#         img = img.transpose(1, 2, 0)\n",
    "#         plt.imshow(img)\n",
    "#         plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2000, out_features=8192, bias=False)\n",
       "    (1): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "  )\n",
       "  (upsample1): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU(inplace)\n",
       "  )\n",
       "  (upsample2): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU(inplace)\n",
       "  )\n",
       "  (upsample3): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU(inplace)\n",
       "  )\n",
       "  (upsample4): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU(inplace)\n",
       "  )\n",
       "  (image): Sequential(\n",
       "    (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = Generator(CONFIG).to(DEVICE)\n",
    "G.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
       "  )\n",
       "  (get_cond_logits): DiscriminatorLogits(\n",
       "    (outlogits): Sequential(\n",
       "      (0): Conv2d(576, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2, inplace)\n",
       "      (3): Conv2d(512, 1, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (4): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = Discriminator(CONFIG).to(DEVICE)\n",
    "D.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = train_loader\n",
    "\n",
    "optimizer_d = torch.optim.Adam(D.parameters(), lr=LR_D, weight_decay=WEIGHT_DECAY)\n",
    "optimizer_g = torch.optim.Adam(G.parameters(), lr=LR_G, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "loss = nn.BCELoss().to(DEVICE)\n",
    "loss_ls = lambda x, y: 0.5 * torch.mean((x - y) ** 2)\n",
    "loss_ms = nn.MSELoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azmi/miniconda3/envs/pytorch1.3/lib/python3.7/site-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([512])) that is different to the input size (torch.Size([4194304])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target and input must have the same number of elements. target nelement (512) != input nelement (4194304)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-1f6abc920bb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moutput_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mloss_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch1.3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch1.3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch1.3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m         raise ValueError(\"Target and input must have the same number of elements. target nelement ({}) \"\n\u001b[0;32m-> 2106\u001b[0;31m                          \"!= input nelement ({})\".format(target.numel(), input.numel()))\n\u001b[0m\u001b[1;32m   2107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Target and input must have the same number of elements. target nelement (512) != input nelement (4194304)"
     ]
    }
   ],
   "source": [
    "G.train()\n",
    "D.train()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    total_g_loss = 0.0\n",
    "    total_d_loss = 0.0\n",
    "    \n",
    "    for i, (images, word_vectors_tensor) in enumerate(data_loader):\n",
    "\n",
    "        batch_size = images.size()[0]\n",
    "        \n",
    "        real_label = torch.full((batch_size,), 1.0, device=DEVICE)\n",
    "        fake_label = torch.full((batch_size,), 0.0, device=DEVICE)\n",
    "\n",
    "        images = images.to(DEVICE)\n",
    "        word_vectors_tensor = word_vectors_tensor.to(DEVICE)\n",
    "        word_sequence = word_vectors_tensor[:, 0, :].unsqueeze(2).unsqueeze(3)\n",
    "        \n",
    "        # Discriminator pass for real\n",
    "        D.zero_grad()\n",
    "        output_real = D(images).view(-1)\n",
    "        loss_real = loss(output_real, real_label)\n",
    "        loss_real.backward(retain_graph=False)\n",
    "        \n",
    "        # Discriminator pass for fake\n",
    "        fake = G(word_sequence)\n",
    "        output_fake = D(fake.detach()).view(-1)\n",
    "        loss_fake = loss(output_fake, fake_label)\n",
    "        loss_fake.backward(retain_graph=False)\n",
    "        loss_d = loss_real + loss_fake\n",
    "        \n",
    "        # Discriminator update\n",
    "        optimizer_d.zero_grad()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Generator pass\n",
    "        G.zero_grad()\n",
    "        output_fake = D(fake).view(-1)\n",
    "        loss_g = loss(output_fake, real_label)\n",
    "        loss_g.backward(retain_graph=False)\n",
    "\n",
    "        # Generator backward pass\n",
    "        optimizer_g.zero_grad()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        # Update total loss\n",
    "        total_g_loss += loss_g.item()\n",
    "        total_d_loss += loss_d.item()\n",
    "\n",
    "        # Print logs\n",
    "        if i % 20 == 0:\n",
    "            print('[{0:3d}/{1}] {2:3d}/{3} loss_g: {4:.4f} | loss_d: {5:4f}'\n",
    "                .format(epoch + 1, N_EPOCHS, i + 1, len(data_loader), loss_g.item(), loss_d.item()))\n",
    "            \n",
    "    print(\"Epoch time: {}\".format(time.time() - epoch_start))\n",
    "            \n",
    "    break\n",
    "            \n",
    "    \n",
    "#     # Save your model weights\n",
    "#     if (epoch + 1) % 5 == 0:\n",
    "#         save_dict = {\n",
    "#             'g':G.state_dict(), \n",
    "#             'g_optim':optimizer_g.state_dict(),\n",
    "#             'd': D.state_dict(),\n",
    "#             'd_optim': optimizer_d.state_dict()\n",
    "#         }\n",
    "#         torch.save(save_dict, os.path.join(MODEL_PATH, 'checkpoint_{}.pth'.format(epoch + 1)))\n",
    "        \n",
    "#     # Merge noisy input, ground truth and network output so that you can compare your results side by side\n",
    "#     out = torch.cat([img, fake], dim=2).detach().cpu().clamp(0.0, 1.0)\n",
    "#     vutils.save_image(out, os.path.join(OUTPUT_PATH, \"{}_{}.png\".format(epoch, i)), normalize=True)\n",
    "    \n",
    "#     # Calculate avarage loss for the current epoch\n",
    "#     avg_g_loss = total_g_loss / len(data_loader)\n",
    "#     avg_d_loss = total_d_loss / len(data_loader)\n",
    "#     print('Epoch[{}] Training Loss G: {:4f} | D: {:4f}'.format(epoch + 1, avg_g_loss, avg_d_loss))\n",
    "    \n",
    "#     cache_train_g.append(avg_g_loss)\n",
    "#     cache_train_d.append(avg_d_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.81770833333333"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "41943 / 576"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.3",
   "language": "python",
   "name": "pytorch1.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
