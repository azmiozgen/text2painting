{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries\n",
    "import glob\n",
    "from multiprocessing import cpu_count\n",
    "import os\n",
    "import sys\n",
    "\n",
    "## 3rd party\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFilter\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "_path = \"..\"\n",
    "if _path not in sys.path:\n",
    "    sys.path.append(_path)\n",
    "from lib.dataset import TextArtDataLoader, AlignCollate\n",
    "from lib.config import Config\n",
    "# from lib.preprocess import (pad_image, crop_edges_lr, )\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD2VEC_MODEL_FILE = \"../models/deviant_wiki_word2vec.model\"\n",
    "BATCH_SIZE = 4\n",
    "# N_WORKERS = cpu_count() - 1\n",
    "N_WORKERS = 1\n",
    "CONFIG = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextArtDataLoader(['wikiart', 'deviantart'], WORD2VEC_MODEL_FILE, mode='train')\n",
    "train_align_collate = AlignCollate('train',\n",
    "                                   CONFIG.MEAN,\n",
    "                                   CONFIG.STD,\n",
    "                                   CONFIG.IMAGE_SIZE_HEIGHT,\n",
    "                                   CONFIG.IMAGE_SIZE_WIDTH,\n",
    "                                   horizontal_flipping=CONFIG.HORIZONTAL_FLIPPING,\n",
    "                                   random_rotation=CONFIG.RANDOM_ROTATION,\n",
    "                                   color_jittering=CONFIG.COLOR_JITTERING,\n",
    "                                   random_grayscale=CONFIG.RANDOM_GRAYSCALE,\n",
    "                                   random_channel_swapping=CONFIG.RANDOM_CHANNEL_SWAPPING,\n",
    "                                   random_gamma=CONFIG.RANDOM_GAMMA,\n",
    "                                   random_resolution=CONFIG.RANDOM_RESOLUTION)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          num_workers=N_WORKERS,\n",
    "                          pin_memory=True,\n",
    "                          collate_fn=train_align_collate,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.8240e-02, -9.8872e-02, -1.8374e-01,  ..., -1.4571e-04,\n",
       "          2.6842e-02, -7.4910e-02],\n",
       "        [-3.4046e-02, -5.9622e-03,  1.7901e-01,  ...,  8.5811e-02,\n",
       "          1.6756e-01,  2.8669e-01],\n",
       "        [-2.2918e-01,  2.0970e-01, -2.4397e-01,  ..., -1.8154e-01,\n",
       "          2.7347e-01, -1.7227e-01],\n",
       "        [-7.2069e-02,  2.3292e-02,  8.6588e-03,  ..., -7.4428e-02,\n",
       "         -7.1368e-02, -2.0730e-02],\n",
       "        [-1.6651e-01,  9.0724e-02, -1.2495e-01,  ..., -2.6451e-01,\n",
       "         -9.3857e-02, -1.0939e-01]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE\n",
      "BATCH: torch.Size([3, 2000])\n",
      "BATCH: torch.Size([4, 2000])\n",
      "BATCH: torch.Size([6, 2000])\n",
      "BATCH: torch.Size([3, 2000])\n",
      "MAX LEN: 6\n",
      "\tSHAPE: torch.Size([6, 2000])\n",
      "\tSHAPE: torch.Size([6, 2000])\n",
      "\tSHAPE: torch.Size([6, 2000])\n",
      "\tSHAPE: torch.Size([6, 2000])\n",
      "HERE\n",
      "BATCH: torch.Size([6, 2000])\n",
      "BATCH: torch.Size([3, 2000])\n",
      "BATCH: torch.Size([5, 2000])\n",
      "BATCH: torch.Size([13, 2000])\n",
      "MAX LEN: 13\n",
      "\tSHAPE: torch.Size([13, 2000])\n",
      "\tSHAPE: torch.Size([13, 2000])\n",
      "\tSHAPE: torch.Size([13, 2000])\n",
      "\tSHAPE: torch.Size([13, 2000])\n",
      "IMAGE: torch.Size([4, 3, 72, 192])\n",
      "WV: torch.Size([4, 6, 2000])\n"
     ]
    }
   ],
   "source": [
    "# for image, label, image_file in train_loader:\n",
    "# for label_sentence, image_file in train_loader:\n",
    "for image, word_vectors_tensor in train_loader:\n",
    "#     print(label_sentence, image_file)\n",
    "    print(\"IMAGE:\", image.shape)\n",
    "    print(\"WV:\", word_vectors_tensor.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE\n",
      "BATCH: torch.Size([10, 2000])\n",
      "BATCH: torch.Size([7, 2000])\n",
      "BATCH: torch.Size([4, 2000])\n",
      "BATCH: torch.Size([4, 2000])\n",
      "MAX LEN: 10\n",
      "\tSHAPE: torch.Size([10, 2000])\n",
      "\tSHAPE: torch.Size([10, 2000])\n",
      "\tSHAPE: torch.Size([10, 2000])\n",
      "\tSHAPE: torch.Size([10, 2000])\n",
      "HERE\n",
      "BATCH: torch.Size([16, 2000])\n",
      "BATCH: torch.Size([4, 2000])\n",
      "BATCH: torch.Size([19, 2000])\n",
      "BATCH: torch.Size([4, 2000])\n",
      "MAX LEN: 19\n",
      "\tSHAPE: torch.Size([19, 2000])\n",
      "\tSHAPE: torch.Size([19, 2000])\n",
      "\tSHAPE: torch.Size([19, 2000])\n",
      "\tSHAPE: torch.Size([19, 2000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/canopus/miniconda3/envs/pytorch-cpu/lib/python3.7/site-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE\n",
      "BATCH: torch.Size([3, 2000])\n",
      "BATCH: torch.Size([3, 2000])\n",
      "BATCH: torch.Size([3, 2000])\n",
      "BATCH: torch.Size([5, 2000])\n",
      "MAX LEN: 5\n",
      "\tSHAPE: torch.Size([5, 2000])\n",
      "\tSHAPE: torch.Size([5, 2000])\n",
      "\tSHAPE: torch.Size([5, 2000])\n",
      "\tSHAPE: torch.Size([5, 2000])\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(train_loader)\n",
    "image, word_vectors = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[-8, -9, -1], \n",
    "                  [-1, 2, -7]])\n",
    "b = torch.Tensor([[-8, -9, -1], \n",
    "                  [-1, 2, -7],\n",
    "                  [1, 2, 7],\n",
    "                  [-3, 2, 2],\n",
    "                  [5, 5, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor(np.pad(b, ((0, len(b) - len(b)), (0, 0))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cpu",
   "language": "python",
   "name": "pytorch-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
