{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries\n",
    "import glob\n",
    "from multiprocessing import cpu_count\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "## 3rd party\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "_path = \"..\"\n",
    "if _path not in sys.path:\n",
    "    sys.path.append(_path)\n",
    "from lib.dataset import TextArtDataLoader, AlignCollate, ImageBatchSampler\n",
    "from lib.config import Config\n",
    "# from lib.preprocess import (pad_image, crop_edges_lr, )\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "# N_WORKERS = cpu_count() - 1\n",
    "N_WORKERS = 4\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "N_EPOCHS = 10\n",
    "LR_G = 1e-4\n",
    "LR_D = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "CONFIG = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextArtDataLoader('united',\n",
    "                                  CONFIG.WORD2VEC_MODEL_FILE,\n",
    "                                  mode='train',\n",
    "                                  load_word_vectors=CONFIG.LOAD_WORD_VECTORS)\n",
    "val_dataset = TextArtDataLoader('united',\n",
    "                                CONFIG.WORD2VEC_MODEL_FILE,\n",
    "                                mode='val',\n",
    "                                load_word_vectors=CONFIG.LOAD_WORD_VECTORS)\n",
    "test_dataset = TextArtDataLoader('united',\n",
    "                                 CONFIG.WORD2VEC_MODEL_FILE,\n",
    "                                 mode='test',\n",
    "                                 load_word_vectors=CONFIG.LOAD_WORD_VECTORS)\n",
    "\n",
    "train_align_collate = AlignCollate('train',\n",
    "                                   CONFIG.WORD2VEC_MODEL_FILE,\n",
    "                                   CONFIG.MEAN,\n",
    "                                   CONFIG.STD,\n",
    "                                   CONFIG.IMAGE_SIZE_HEIGHT,\n",
    "                                   CONFIG.IMAGE_SIZE_WIDTH,\n",
    "                                   horizontal_flipping=CONFIG.HORIZONTAL_FLIPPING,\n",
    "                                   random_rotation=CONFIG.RANDOM_ROTATION,\n",
    "                                   color_jittering=CONFIG.COLOR_JITTERING,\n",
    "                                   random_grayscale=CONFIG.RANDOM_GRAYSCALE,\n",
    "                                   random_channel_swapping=CONFIG.RANDOM_CHANNEL_SWAPPING,\n",
    "                                   random_gamma=CONFIG.RANDOM_GAMMA,\n",
    "                                   random_resolution=CONFIG.RANDOM_RESOLUTION,\n",
    "                                   word_vectors_similar_pad=CONFIG.WORD_VECTORS_SIMILAR_PAD,\n",
    "                                   word_vectors_similar_pad_topN=CONFIG.WORD_VECTORS_SIMILAR_PAD_TOPN)\n",
    "val_align_collate = AlignCollate('val',\n",
    "                                 CONFIG.WORD2VEC_MODEL_FILE,\n",
    "                                 CONFIG.MEAN,\n",
    "                                 CONFIG.STD,\n",
    "                                 CONFIG.IMAGE_SIZE_HEIGHT,\n",
    "                                 CONFIG.IMAGE_SIZE_WIDTH,\n",
    "                                 horizontal_flipping=CONFIG.HORIZONTAL_FLIPPING,\n",
    "                                 random_rotation=CONFIG.RANDOM_ROTATION,\n",
    "                                 color_jittering=CONFIG.COLOR_JITTERING,\n",
    "                                 random_grayscale=CONFIG.RANDOM_GRAYSCALE,\n",
    "                                 random_channel_swapping=CONFIG.RANDOM_CHANNEL_SWAPPING,\n",
    "                                 random_gamma=CONFIG.RANDOM_GAMMA,\n",
    "                                 random_resolution=CONFIG.RANDOM_RESOLUTION,\n",
    "                                 word_vectors_similar_pad=CONFIG.WORD_VECTORS_SIMILAR_PAD,\n",
    "                                 word_vectors_similar_pad_topN=CONFIG.WORD_VECTORS_SIMILAR_PAD_TOPN)\n",
    "\n",
    "train_batch_sampler = ImageBatchSampler('united', BATCH_SIZE, shuffle_groups=True, mode='train')\n",
    "val_batch_sampler = ImageBatchSampler('united', BATCH_SIZE, shuffle_groups=True, mode='val')\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False,\n",
    "                          num_workers=N_WORKERS,\n",
    "                          pin_memory=True,\n",
    "                          collate_fn=train_align_collate,\n",
    "                          sampler=train_batch_sampler,\n",
    "                         )\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False,\n",
    "                          num_workers=N_WORKERS,\n",
    "                          pin_memory=True,\n",
    "                          collate_fn=val_align_collate,\n",
    "                          sampler=val_batch_sampler\n",
    "                         )\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False,\n",
    "                          num_workers=N_WORKERS,\n",
    "                          pin_memory=True,\n",
    "                          collate_fn=None,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i, (image, word_vectors_tensor) in enumerate(train_loader):\n",
    "#     print(\"IMAGE:\", image.shape)\n",
    "#     print(\"WV:\", word_vectors_tensor.shape)\n",
    "    \n",
    "#     for word_vectors in word_vectors_tensor:\n",
    "#         for word_vector in word_vectors:\n",
    "#             wv = np.array(word_vector)\n",
    "#             word, prob = train_loader.dataset.word2vec_model.wv.similar_by_vector(wv)[0]\n",
    "#             if prob > 0.95:\n",
    "#                 print(word, end=',')\n",
    "#         print()\n",
    "    \n",
    "#     images = np.array(image)\n",
    "#     for img in images:\n",
    "#         img = img.transpose(1, 2, 0)\n",
    "#         plt.imshow(img)\n",
    "#         plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (5): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (8): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (11): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = Discriminator().to(DEVICE)\n",
    "D.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (main): Sequential(\n",
       "    (0): ConvTranspose2d(2000, 64, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace)\n",
       "    (9): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace)\n",
       "    (12): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = Generator().to(DEVICE)\n",
    "G.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = train_loader\n",
    "\n",
    "optimizer_d = torch.optim.Adam(D.parameters(), lr=LR_D, weight_decay=WEIGHT_DECAY)\n",
    "optimizer_g = torch.optim.Adam(G.parameters(), lr=LR_G, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "loss = nn.BCELoss().to(DEVICE)\n",
    "loss_ls = lambda x, y: 0.5 * torch.mean((x - y) ** 2)\n",
    "loss_ms = nn.MSELoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/10]   1/125 loss_g: 0.6816 | loss_d: 1.448273\n",
      "[  1/10]  21/125 loss_g: 0.6456 | loss_d: 1.451247\n",
      "[  1/10]  41/125 loss_g: 0.6509 | loss_d: 1.500129\n",
      "[  1/10]  61/125 loss_g: 0.6568 | loss_d: 1.473855\n",
      "[  1/10]  81/125 loss_g: 0.6750 | loss_d: 1.416752\n",
      "[  1/10] 101/125 loss_g: 0.6148 | loss_d: 1.490674\n",
      "Epoch time: 922.9307477474213\n"
     ]
    }
   ],
   "source": [
    "G.train()\n",
    "D.train()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    total_g_loss = 0.0\n",
    "    total_d_loss = 0.0\n",
    "    \n",
    "    for i, (images, word_vectors_tensor) in enumerate(data_loader):\n",
    "\n",
    "        batch_size = images.size()[0]\n",
    "        \n",
    "        real_label = torch.full((batch_size,), 1.0, device=DEVICE)\n",
    "        fake_label = torch.full((batch_size,), 0.0, device=DEVICE)\n",
    "\n",
    "        images = images.to(DEVICE)\n",
    "        word_vectors_tensor = word_vectors_tensor.to(DEVICE)\n",
    "        word_sequence = word_vectors_tensor[:, 0, :].unsqueeze(2).unsqueeze(3)\n",
    "        \n",
    "        # Discriminator pass for real\n",
    "        D.zero_grad()\n",
    "        output_real = D(images).view(-1)\n",
    "        loss_real = loss(output_real, real_label)\n",
    "        loss_real.backward(retain_graph=False)\n",
    "        \n",
    "        # Discriminator pass for fake\n",
    "        fake = G(word_sequence)\n",
    "        output_fake = D(fake.detach()).view(-1)\n",
    "        loss_fake = loss(output_fake, fake_label)\n",
    "        loss_fake.backward(retain_graph=False)\n",
    "        loss_d = loss_real + loss_fake\n",
    "        \n",
    "        # Discriminator update\n",
    "        optimizer_d.zero_grad()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Generator pass\n",
    "        G.zero_grad()\n",
    "        output_fake = D(fake).view(-1)\n",
    "        loss_g = loss(output_fake, real_label)\n",
    "        loss_g.backward(retain_graph=False)\n",
    "\n",
    "        # Generator backward pass\n",
    "        optimizer_g.zero_grad()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        # Update total loss\n",
    "        total_g_loss += loss_g.item()\n",
    "        total_d_loss += loss_d.item()\n",
    "\n",
    "        # Print logs\n",
    "        if i % 20 == 0:\n",
    "            print('[{0:3d}/{1}] {2:3d}/{3} loss_g: {4:.4f} | loss_d: {5:4f}'\n",
    "                .format(epoch + 1, N_EPOCHS, i + 1, len(data_loader), loss_g.item(), loss_d.item()))\n",
    "            \n",
    "    print(\"Epoch time: {}\".format(time.time() - epoch_start))\n",
    "            \n",
    "    break\n",
    "            \n",
    "    \n",
    "#     # Save your model weights\n",
    "#     if (epoch + 1) % 5 == 0:\n",
    "#         save_dict = {\n",
    "#             'g':G.state_dict(), \n",
    "#             'g_optim':optimizer_g.state_dict(),\n",
    "#             'd': D.state_dict(),\n",
    "#             'd_optim': optimizer_d.state_dict()\n",
    "#         }\n",
    "#         torch.save(save_dict, os.path.join(MODEL_PATH, 'checkpoint_{}.pth'.format(epoch + 1)))\n",
    "        \n",
    "#     # Merge noisy input, ground truth and network output so that you can compare your results side by side\n",
    "#     out = torch.cat([img, fake], dim=2).detach().cpu().clamp(0.0, 1.0)\n",
    "#     vutils.save_image(out, os.path.join(OUTPUT_PATH, \"{}_{}.png\".format(epoch, i)), normalize=True)\n",
    "    \n",
    "#     # Calculate avarage loss for the current epoch\n",
    "#     avg_g_loss = total_g_loss / len(data_loader)\n",
    "#     avg_d_loss = total_d_loss / len(data_loader)\n",
    "#     print('Epoch[{}] Training Loss G: {:4f} | D: {:4f}'.format(epoch + 1, avg_g_loss, avg_d_loss))\n",
    "    \n",
    "#     cache_train_g.append(avg_g_loss)\n",
    "#     cache_train_d.append(avg_d_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.3",
   "language": "python",
   "name": "pytorch1.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
